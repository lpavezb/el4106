# -*- coding: utf-8 -*-
"""prueba_tf_3_v07_alumnos.ipynb

Automatically generated by Colaboratory.
"""

#https://medium.com/@curiousily/tensorflow-for-hackers-part-ii-building-simple-neural-network-2d6779d2f91b
import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from math import floor, ceil
from pylab import rcParams
from sklearn.preprocessing import LabelBinarizer
import time

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
# %matplotlib inline


def split_data(data, train_size=0.8):
    size = len(data[0])  # number of characteristics + class number
    n_classes = int(max(data[:, size-1]))  # number of classes
    classes = [np.zeros(size)]*n_classes  # each class start with a array of 0s (for vstack function)
    for x in data:
        classes[int(x[size-1])-1] = np.vstack((classes[int(x[size-1])-1], x))  # example: if x belong to class 1, x is stored in classes[0]

    train = [np.zeros(size)]*n_classes  # the set start with a array of 0s (for vstack function)
    validation = [np.zeros(size)]*n_classes  # the set start with a array of 0s (for vstack function)
    for i in range(n_classes):
        classes[i] = classes[i][1:]  # delete the first row of each class matrix (the 0s row for vstack)
        np.random.shuffle(classes[i])  # shuffle data
        train_len = int(len(classes[i]) * train_size)  # calculate size of train set
        train = np.vstack((train, np.array(classes[i][:train_len])))  # append a new class matrix to the train matrix
        validation = np.vstack((validation, np.array(classes[i][train_len:])))  # append a new class matrix to the validation matrix

    train = train[1:]  # delete first row
    validation = validation[1:]  # delete first row
    np.random.shuffle(train)  # the data is ordered by class
    np.random.shuffle(validation)
    return train, validation


def multilayer_perceptron(x, weights, biases, keep_prob):
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_1 = tf.nn.sigmoid(layer_1)
    layer_1 = tf.nn.dropout(layer_1, keep_prob)
    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']
    return out_layer


random_state = 42
np.random.seed(random_state)
tf.set_random_seed(random_state)


D = np.loadtxt('sensorless_tarea2_train.txt', delimiter=',')
T = np.loadtxt('sensorless_tarea2_test.txt', delimiter=',')

train, validation = split_data(D)
nc = D.shape[1]-1
data_x = D[:, :nc]
data_y = D[:, nc] - 1
data_y = data_y.astype(int)

x_train = train[:, :nc]
y_train = train[:, nc] - 1
y_train = y_train.astype(int)

x_valid = validation[:, :nc]
y_valid = validation[:, nc] - 1
y_valid = y_valid.astype(int)


label_binarizer = LabelBinarizer()
label_binarizer.fit(range(max(data_y)+1))
data_y = label_binarizer.transform(data_y).astype(float)
y_train = label_binarizer.transform(y_train).astype(float)
y_valid = label_binarizer.transform(y_valid).astype(float)


n_hidden_1 = 28
n_input = data_x.shape[1]
n_classes = data_y.shape[1]

weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))
}

biases = {
    'b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}

keep_prob = tf.placeholder("float")

training_epochs = 1500
display_step = 100
batch_size = 32

x = tf.placeholder("float", [None, n_input])
y = tf.placeholder("float", [None, n_classes])

predictions = multilayer_perceptron(x, weights, biases, keep_prob)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))

optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    t1 = time.time()
    acc = []
    for epoch in range(training_epochs):
        avg_cost = 0.0
        total_batch = int(len(x_train) / batch_size)
        x_batches = np.array_split(x_train, total_batch)
        y_batches = np.array_split(y_train, total_batch)
        for i in range(total_batch):
            batch_x, batch_y = x_batches[i], y_batches[i]
            _, c = sess.run([optimizer, cost],
                            feed_dict={
                                x: batch_x,
                                y: batch_y,
                                keep_prob: 0.8
                            })
            avg_cost += c / total_batch
        correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        accuracy_eval = accuracy.eval({x: x_valid, y: y_valid, keep_prob: 1.0})
        acc.append(accuracy_eval)
        if epoch % display_step == 0:
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))
            print("Accuracy validation:", accuracy_eval)
    t2 = time.time()
    print("Optimization Finished!, time = {:.2f}".format(t2-t1))
    plt.scatter(range(training_epochs), acc)
    plt.title("accuracy curve")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.show()
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    print("Accuracy validation:", accuracy.eval({x: x_valid, y: y_valid, keep_prob: 1.0}))
    print("Confusion matrix validation")
    confm = tf.confusion_matrix(tf.argmax(y,1),tf.argmax(predictions, 1), num_classes=y_valid.shape[1])
    print(confm.eval({x: x_valid, y: y_valid, keep_prob: 1.0}))
